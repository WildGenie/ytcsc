{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Dropout, Activation, Input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_trans_stem = pd.read_csv('../data/preproc_no_trans_stem.csv')\n",
    "data_trans = pd.read_csv('../data/preproc_trans.csv')\n",
    "data_stem = pd.read_csv('../data/preproc_stem.csv')\n",
    "data_trans_stem = pd.read_csv('../data/preproc_trans_stem.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, make sure the negative and positive comments are even in numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    778\n",
       "1    555\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_no_trans_stem.rating.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are more negatives, drop random negative sentiment comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_indices = data_no_trans_stem.index[data_no_trans_stem.rating == 0].tolist()\n",
    "diff = abs(np.diff(data_no_trans_stem.rating.value_counts().values)[0])\n",
    "indices = np.random.choice(negative_indices, diff, replace=False)\n",
    "data_no_trans_stem = data_no_trans_stem.drop(indices)\n",
    "data_trans = data_trans.drop(indices)\n",
    "data_stem = data_stem.drop(indices)\n",
    "data_trans_stem = data_trans_stem.drop(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_no_trans_stem = [[word for word in str(body).split()] for body in data_no_trans_stem.body]\n",
    "sentences_trans = [[word for word in str(body).split()] for body in data_trans.body]\n",
    "sentences_stem = [[word for word in str(body).split()] for body in data_stem.body]\n",
    "sentences_trans_stem = [[word for word in str(body).split()] for body in data_trans_stem.body]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "seed = 1234\n",
    "min_word_count = 1\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec model based on all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(\n",
    "sentences=sentences_no_trans_stem+sentences_trans+sentences_stem+sentences_trans_stem,\n",
    "seed=seed,\n",
    "min_count=min_word_count,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\indre\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('from', 0.9999276995658875),\n",
       " ('and', 0.9999189972877502),\n",
       " ('that', 0.9999186992645264),\n",
       " ('are', 0.9999184608459473),\n",
       " ('with', 0.9999182224273682),\n",
       " ('when', 0.9999178647994995),\n",
       " ('up', 0.9999163150787354),\n",
       " ('have', 0.9999160170555115),\n",
       " ('in', 0.9999159574508667),\n",
       " ('for', 0.9999144077301025)]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.most_similar('love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = word2vec.wv.vectors\n",
    "vocab_size, emdedding_size = word2vec.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer based on all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.zeros((vocab_size, emdedding_size))\n",
    "for word, i in words.items():\n",
    "    if word in word2vec.wv.vocab:\n",
    "        embeddings[i-1] = word2vec.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(X_train, Y_train, X_val, Y_val, es_patience=10, epochs=100, batch_size=128, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size, weights=[word2vec.wv.vectors]))\n",
    "    model.add(LSTM(emdedding_size, activation='sigmoid', return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(64, activation='sigmoid'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer=optimizer, metrics = ['accuracy'])\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=es_patience)\n",
    "    hist = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), \n",
    "                 epochs=epochs, batch_size=batch_size, verbose = 4, callbacks=[early_stopping])\n",
    "    return model, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tokenizer.texts_to_sequences(sentences_stem)\n",
    "X = pad_sequences(X)\n",
    "Y = data_trans_stem.rating.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = random_state)\n",
    "#X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state = random_state)\n",
    "index = int(len(X) * 0.7)\n",
    "# train set\n",
    "X_t = X[:index]\n",
    "Y_t = Y[:index]\n",
    "# test set\n",
    "X_tt = X[index:]\n",
    "Y_tt = Y[index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# grid search parameters\n",
    "optimizers = ['rmsprop', 'adam']\n",
    "es_patience = [2, 6, 10]\n",
    "epochs = [50]\n",
    "batch_sizes = [8, 16, 32, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_results = {\n",
    "    'optimizer': '',\n",
    "    'es_patience': 0,\n",
    "    'epochs': 0,\n",
    "    'batches': 0\n",
    "}\n",
    "curr_acc = 0\n",
    "curr_loss = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation setup\n",
    "sk = StratifiedKFold(n_splits = 3, random_state = random_state, shuffle = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer:  rmsprop , ES:  2 , epochs:  50 , batches:  8\n",
      "Optimizer:  rmsprop , ES:  2 , epochs:  50 , batches:  16\n",
      "Optimizer:  rmsprop , ES:  2 , epochs:  50 , batches:  32\n",
      "Optimizer:  rmsprop , ES:  2 , epochs:  50 , batches:  64\n",
      "Optimizer:  rmsprop , ES:  6 , epochs:  50 , batches:  8\n",
      "Optimizer:  rmsprop , ES:  6 , epochs:  50 , batches:  16\n",
      "Optimizer:  rmsprop , ES:  6 , epochs:  50 , batches:  32\n",
      "Optimizer:  rmsprop , ES:  6 , epochs:  50 , batches:  64\n",
      "Optimizer:  rmsprop , ES:  10 , epochs:  50 , batches:  8\n",
      "Optimizer:  rmsprop , ES:  10 , epochs:  50 , batches:  16\n",
      "Optimizer:  rmsprop , ES:  10 , epochs:  50 , batches:  32\n",
      "Optimizer:  rmsprop , ES:  10 , epochs:  50 , batches:  64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer:  adam , ES:  2 , epochs:  50 , batches:  8\n",
      "Optimizer:  adam , ES:  2 , epochs:  50 , batches:  16\n",
      "Optimizer:  adam , ES:  2 , epochs:  50 , batches:  32\n",
      "Optimizer:  adam , ES:  2 , epochs:  50 , batches:  64\n",
      "Optimizer:  adam , ES:  6 , epochs:  50 , batches:  8\n",
      "Optimizer:  adam , ES:  6 , epochs:  50 , batches:  16\n",
      "Optimizer:  adam , ES:  6 , epochs:  50 , batches:  32\n",
      "Optimizer:  adam , ES:  6 , epochs:  50 , batches:  64\n",
      "Optimizer:  adam , ES:  10 , epochs:  50 , batches:  8\n",
      "Optimizer:  adam , ES:  10 , epochs:  50 , batches:  16\n",
      "Optimizer:  adam , ES:  10 , epochs:  50 , batches:  32\n",
      "Optimizer:  adam , ES:  10 , epochs:  50 , batches:  64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
     ]
    }
   ],
   "source": [
    "#Gridsearch\n",
    "for o in optimizers:\n",
    "    for es in es_patience:\n",
    "        for e in epochs:\n",
    "            for b in batch_sizes:\n",
    "                print('Optimizer: ', o, ', ES: ', es, ', epochs: ', e, ', batches: ', b)\n",
    "                #cross validation\n",
    "                for train_index, val_index in sk.split(X_t,Y_t):\n",
    "                    X_train, X_val = X_t[train_index], X_t[val_index]\n",
    "                    y_train, y_val = Y_t[train_index], Y_t[val_index]\n",
    "                    model, hist = build_model(X_train, y_train, X_val, y_val, es, e, b, o)\n",
    "                    loss, acc = model.evaluate(X_val, y_val, verbose = 4, batch_size = 32)\n",
    "                    if acc > curr_acc:\n",
    "                        best_results['optimizer'] = o\n",
    "                        best_results['es_patience'] = es\n",
    "                        best_results['epochs'] = e\n",
    "                        best_results['batches'] = b\n",
    "                        curr_acc = acc\n",
    "                        curr_loss = loss\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'optimizer': 'rmsprop', 'es_patience': 6, 'epochs': 50, 'batches': 16}"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7567567825317383"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6021022796630859"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
