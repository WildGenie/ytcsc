{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, Dropout, Activation, Input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_trans_stem = pd.read_csv('../data/preproc_no_trans_stem.csv')\n",
    "data_trans = pd.read_csv('../data/preproc_trans.csv')\n",
    "data_stem = pd.read_csv('../data/preproc_stem.csv')\n",
    "data_trans_stem = pd.read_csv('../data/preproc_trans_stem.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, make sure the negative and positive comments are even in numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1698\n",
       "1    1204\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_no_trans_stem.rating.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are more negatives, drop random negative sentiment comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_indices = data_no_trans_stem.index[data_no_trans_stem.rating == 0].tolist()\n",
    "diff = abs(np.diff(data_no_trans_stem.rating.value_counts().values)[0])\n",
    "indices = np.random.choice(negative_indices, diff, replace=False)\n",
    "data_no_trans_stem = data_no_trans_stem.drop(indices)\n",
    "data_trans = data_trans.drop(indices)\n",
    "data_stem = data_stem.drop(indices)\n",
    "data_trans_stem = data_trans_stem.drop(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_no_trans_stem = [[word for word in str(body).split()] for body in data_no_trans_stem.body]\n",
    "sentences_trans = [[word for word in str(body).split()] for body in data_trans.body]\n",
    "sentences_stem = [[word for word in str(body).split()] for body in data_stem.body]\n",
    "sentences_trans_stem = [[word for word in str(body).split()] for body in data_trans_stem.body]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "seed = 1234\n",
    "min_word_count = 1\n",
    "random_state = 42\n",
    "word_vectors_file = 'GoogleNews-vectors-negative300.bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec model based on all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word2vec = KeyedVectors.load_word2vec_format(word_vectors_file, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(\n",
    "#sentences=sentences_no_trans_stem+sentences_trans+sentences_stem+sentences_trans_stem,\n",
    "sentences=sentences_no_trans_stem,\n",
    "seed=seed,\n",
    "min_count=min_word_count,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('just', 0.9971455335617065),\n",
       " ('video', 0.9971255660057068),\n",
       " ('why', 0.9971076846122742),\n",
       " ('he', 0.9970792531967163),\n",
       " ('was', 0.9970666170120239),\n",
       " ('for', 0.9969887733459473),\n",
       " ('like', 0.9969620704650879),\n",
       " ('not', 0.9968778491020203),\n",
       " ('your', 0.9968664646148682),\n",
       " ('so', 0.9968242049217224)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.most_similar('hate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = word2vec.wv.vectors\n",
    "vocab_size, emdedding_size = word2vec.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizer based on all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences_no_trans_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used if pretrained word2vec model is used\n",
    "#embeddings = np.zeros((vocab_size, emdedding_size))\n",
    "#for word, i in words.items():\n",
    "#    if word in word2vec.vocab:\n",
    "#        embeddings[i-1] = word2vec[word]\n",
    "#print('Null word embeddings: ',  np.sum(np.sum(embeddings, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(X_train, Y_train, X_val, Y_val, es_patience=10, epochs=100, batch_size=128, optimizer='adam'):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size, weights=[word2vec.wv.vectors]))\n",
    "    model.add(LSTM(emdedding_size, activation='sigmoid', return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer=optimizer, metrics = ['accuracy'])\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=es_patience)\n",
    "    hist = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), \n",
    "                 epochs=epochs, batch_size=batch_size, verbose = 3, callbacks=[early_stopping])\n",
    "    return model, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tokenizer.texts_to_sequences(sentences_no_trans_stem)\n",
    "X = pad_sequences(X)\n",
    "Y = data_trans_stem.rating.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = random_state)\n",
    "#X_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state = random_state)\n",
    "index = int(len(X) * 0.8)\n",
    "# train set\n",
    "X_t = X[:index]\n",
    "Y_t = Y[:index]\n",
    "# test set\n",
    "X_tt = X[index:]\n",
    "Y_tt = Y[index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# grid search parameters\n",
    "optimizers = ['rmsprop', 'adam']\n",
    "es_patience = [2, 6, 10]\n",
    "epochs = [50]\n",
    "batch_sizes = [8, 16, 32, 64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation setup\n",
    "sk = StratifiedKFold(n_splits = 5, random_state = random_state, shuffle = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Epoch 2/50\n",
      "Epoch 3/50\n",
      "Epoch 4/50\n",
      "Epoch 5/50\n",
      "Epoch 6/50\n",
      "Epoch 7/50\n",
      "Epoch 8/50\n",
      "Epoch 9/50\n",
      "Epoch 10/50\n",
      "Epoch 11/50\n",
      "Epoch 12/50\n",
      "Epoch 13/50\n",
      "Epoch 14/50\n",
      "Epoch 1/50\n",
      "Epoch 2/50\n",
      "Epoch 3/50\n",
      "Epoch 4/50\n",
      "Epoch 5/50\n",
      "Epoch 6/50\n",
      "Epoch 7/50\n",
      "Epoch 8/50\n",
      "Epoch 9/50\n",
      "Epoch 10/50\n",
      "Epoch 11/50\n",
      "Epoch 12/50\n",
      "Epoch 13/50\n",
      "Epoch 1/50\n",
      "Epoch 2/50\n",
      "Epoch 3/50\n",
      "Epoch 4/50\n",
      "Epoch 5/50\n",
      "Epoch 6/50\n",
      "Epoch 7/50\n",
      "Epoch 8/50\n",
      "Epoch 9/50\n",
      "Epoch 10/50\n",
      "Epoch 11/50\n",
      "Epoch 1/50\n",
      "Epoch 2/50\n",
      "Epoch 3/50\n",
      "Epoch 4/50\n",
      "Epoch 5/50\n",
      "Epoch 6/50\n",
      "Epoch 7/50\n",
      "Epoch 8/50\n",
      "Epoch 9/50\n",
      "Epoch 10/50\n",
      "Epoch 11/50\n",
      "Epoch 12/50\n",
      "Epoch 13/50\n",
      "Epoch 1/50\n",
      "Epoch 2/50\n",
      "Epoch 3/50\n",
      "Epoch 4/50\n",
      "Epoch 5/50\n",
      "Epoch 6/50\n",
      "Epoch 7/50\n",
      "Epoch 8/50\n",
      "Epoch 9/50\n",
      "Epoch 10/50\n",
      "Epoch 1/50\n",
      "Epoch 2/50\n",
      "Epoch 3/50\n",
      "Epoch 4/50\n",
      "Epoch 5/50\n",
      "Epoch 6/50\n",
      "Epoch 7/50\n",
      "Epoch 8/50\n",
      "Epoch 9/50\n",
      "Epoch 10/50\n",
      "Epoch 1/50\n",
      "Epoch 2/50\n",
      "Epoch 3/50\n",
      "Epoch 4/50\n",
      "Epoch 5/50\n",
      "Epoch 6/50\n",
      "Epoch 7/50\n",
      "Epoch 8/50\n",
      "Epoch 9/50\n",
      "Epoch 10/50\n",
      "Epoch 11/50\n",
      "Epoch 1/50\n",
      "Epoch 2/50\n",
      "Epoch 3/50\n",
      "Epoch 4/50\n",
      "Epoch 5/50\n",
      "Epoch 6/50\n",
      "Epoch 7/50\n",
      "Epoch 8/50\n",
      "Epoch 9/50\n",
      "Epoch 10/50\n",
      "Epoch 1/50\n",
      "Epoch 2/50\n",
      "Epoch 3/50\n",
      "Epoch 4/50\n",
      "Epoch 5/50\n",
      "Epoch 6/50\n",
      "Epoch 7/50\n",
      "Epoch 8/50\n",
      "Epoch 9/50\n",
      "Epoch 1/50\n",
      "Epoch 2/50\n",
      "Epoch 3/50\n",
      "Epoch 4/50\n",
      "Epoch 5/50\n",
      "Epoch 6/50\n",
      "Epoch 7/50\n",
      "Epoch 8/50\n",
      "Epoch 9/50\n",
      "Epoch 10/50\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for o in optimizers:\n",
    "    optimizer_results = []\n",
    "    for train_index, val_index in sk.split(X_t,Y_t):\n",
    "        X_train, X_val = X_t[train_index], X_t[val_index]\n",
    "        y_train, y_val = Y_t[train_index], Y_t[val_index]\n",
    "        model, hist = build_model(X_train, y_train, X_val, y_val, 6, 50, 16, o)\n",
    "        loss, acc = model.evaluate(X_val, y_val, verbose = 4, batch_size = 32)\n",
    "        optimizer_results.append([loss, acc])\n",
    "    results.append(optimizer_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rmsprop:  0.8100890040397644\n",
      "Average adam:  0.7934718012809754\n"
     ]
    }
   ],
   "source": [
    "# adam vs rmsprop\n",
    "rms_results = np.array(results[0])\n",
    "adam_results = np.array(results[1])\n",
    "print('Average rmsprop: ', np.mean(rms_results[:,1]))\n",
    "print('Average adam: ',  np.mean(adam_results[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_results = {\n",
    "    'optimizer': '',\n",
    "    'es_patience': 0,\n",
    "    'epochs': 0,\n",
    "    'batches': 0\n",
    "}\n",
    "curr_acc = 0\n",
    "curr_loss = 999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer:  rmsprop , ES:  2 , epochs:  50 , batches:  8\n",
      "Optimizer:  rmsprop , ES:  2 , epochs:  50 , batches:  16\n",
      "Optimizer:  rmsprop , ES:  2 , epochs:  50 , batches:  32\n",
      "Optimizer:  rmsprop , ES:  2 , epochs:  50 , batches:  64\n",
      "Optimizer:  rmsprop , ES:  6 , epochs:  50 , batches:  8\n",
      "Optimizer:  rmsprop , ES:  6 , epochs:  50 , batches:  16\n",
      "Optimizer:  rmsprop , ES:  6 , epochs:  50 , batches:  32\n",
      "Optimizer:  rmsprop , ES:  6 , epochs:  50 , batches:  64\n",
      "Optimizer:  rmsprop , ES:  10 , epochs:  50 , batches:  8\n",
      "Optimizer:  rmsprop , ES:  10 , epochs:  50 , batches:  16\n",
      "Optimizer:  rmsprop , ES:  10 , epochs:  50 , batches:  32\n",
      "Optimizer:  rmsprop , ES:  10 , epochs:  50 , batches:  64\n",
      "Optimizer:  adam , ES:  2 , epochs:  50 , batches:  8\n",
      "Optimizer:  adam , ES:  2 , epochs:  50 , batches:  16\n",
      "Optimizer:  adam , ES:  2 , epochs:  50 , batches:  32\n",
      "Optimizer:  adam , ES:  2 , epochs:  50 , batches:  64\n",
      "Optimizer:  adam , ES:  6 , epochs:  50 , batches:  8\n",
      "Optimizer:  adam , ES:  6 , epochs:  50 , batches:  16\n",
      "Optimizer:  adam , ES:  6 , epochs:  50 , batches:  32\n",
      "Optimizer:  adam , ES:  6 , epochs:  50 , batches:  64\n",
      "Optimizer:  adam , ES:  10 , epochs:  50 , batches:  8\n",
      "Optimizer:  adam , ES:  10 , epochs:  50 , batches:  16\n",
      "Optimizer:  adam , ES:  10 , epochs:  50 , batches:  32\n",
      "Optimizer:  adam , ES:  10 , epochs:  50 , batches:  64\n"
     ]
    }
   ],
   "source": [
    "#Gridsearch\n",
    "for o in optimizers:\n",
    "    for es in es_patience:\n",
    "        for e in epochs:\n",
    "            for b in batch_sizes:\n",
    "                print('Optimizer: ', o, ', ES: ', es, ', epochs: ', e, ', batches: ', b)\n",
    "                #cross validation\n",
    "                for train_index, val_index in sk.split(X_t,Y_t):\n",
    "                    X_train, X_val = X_t[train_index], X_t[val_index]\n",
    "                    y_train, y_val = Y_t[train_index], Y_t[val_index]\n",
    "                    model, hist = build_model(X_train, y_train, X_val, y_val, es, e, b, o)\n",
    "                    loss, acc = model.evaluate(X_val, y_val, verbose = 4, batch_size = 32)\n",
    "                    if acc > curr_acc:\n",
    "                        best_results['optimizer'] = o\n",
    "                        best_results['es_patience'] = es\n",
    "                        best_results['epochs'] = e\n",
    "                        best_results['batches'] = b\n",
    "                        curr_acc = acc\n",
    "                        curr_loss = loss\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'optimizer': 'rmsprop', 'es_patience': 6, 'epochs': 50, 'batches': 16}"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7567567825317383"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6021022796630859"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "curr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
