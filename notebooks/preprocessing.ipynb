{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "import enchant\n",
    "from enchant.checker import SpellChecker\n",
    "import enchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version\n",
      "3.7.7 (tags/v3.7.7:d7c567b08f, Mar 10 2020, 10:41:24) [MSC v.1900 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python version\")\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/raw_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the text more uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preproc'] = df.body.replace(r'http\\S+', '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change acronyms to words for uniformity. \n",
    "* https://www.netlingo.com/acronyms.php\n",
    "* https://blog.adioma.com/internet-acronyms-intro-list-infographic/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from acronyms_smileys import acronyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spell checker with uppercase some letters later on\n",
    "df.preproc = df.preproc.str.lower()\n",
    "# the ’ was causing issues, took a while to notice\n",
    "df.preproc = df.preproc.str.replace('’', '\\'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.preproc = df.preproc.apply(lambda x: ' '.join(acronyms.get(word, word) for word in x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix as many spelling errors as possible\n",
    "\n",
    "**TODO:**\n",
    "* either consider another spell checker or skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spell_checker = SpellChecker(\"en_UK\",\"en_US\")\n",
    "#def correct_error(body):\n",
    "#    spell_checker.set_text(body)\n",
    "#    for err in spell_checker:\n",
    "#        if len(err.suggest())>0: \n",
    "#            sug = err.suggest()[0]\n",
    "#            err.replace(sug)\n",
    "#    return spell_checker.get_text()\n",
    "#df.body = df.body.apply(lambda row: correct_error(row))\n",
    "# again, thanks to spell checker\n",
    "#df.body = df.body.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace negations with \"not\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "negations = ['don\\'t', 'aint' 'aren\\'t', 'couldn\\'t','didn\\'t', \n",
    "             'doesn\\'t', 'hadn\\'t', 'hasn\\'t', 'haven\\'t', 'isn\\'t', \n",
    "             'mightn\\'t', 'mustn\\'t', 'needn\\'t', 'shouldn\\'t', 'wasn\\'t', \n",
    "             'weren\\'t', 'won\\'t', 'wouldn\\'t', 'nor', 'not', 'cant', 'dont',\n",
    "            'arent', 'couldnt', 'didnt', 'doesnt', 'hadnt', 'hasnt', 'havent',\n",
    "            'isnt', 'mightnt', 'mustnt', 'neednt', 'shouldnt', 'wasnt',\n",
    "            'werent', 'wont', 'wouldnt']\n",
    "regx = r'\\b(?:{})\\b'.format('|'.join(negations))\n",
    "df.preproc = df.preproc.str.replace(regx, 'not')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove most common stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove negations from stop list, add two missing contractions\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list = [el for el in stopwords_list if el not in negations]\n",
    "missing_words = ['i\\'m', 'i\\'d']\n",
    "stopwords_list.extend(missing_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 most commonly used words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "this    432\n",
       "the     426\n",
       "i       371\n",
       "is      302\n",
       "you     277\n",
       "to      263\n",
       "a       241\n",
       "not     195\n",
       "and     185\n",
       "of      172\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(' '.join(df.preproc).lower().split()).value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_stopwords(stop_words, n=5):\n",
    "    most_freq_words = pd.Series(' '.join(df.preproc).lower().split()).value_counts()[:int(n*2)].keys().to_numpy()\n",
    "    common_stopwords = [i for i in most_freq_words if i in stop_words]\n",
    "    return common_stopwords[0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_stopwords = get_common_stopwords(stopwords_list)\n",
    "df.preproc = df.preproc.apply(lambda x: ' '.join([word for word in x.split() if word not in (common_stopwords)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>rated</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>date</th>\n",
       "      <th>preproc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>*stretched past the 10 minute mark for ad reve...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgztP4lVR-Epv5HlSXN4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2020-01-10T20:24:33Z</td>\n",
       "      <td>*stretched past 10 minute mark for ad revenue,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Big time scam</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgzZubyLG5FtZu7qlal4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2019-08-01T15:45:49Z</td>\n",
       "      <td>big time scam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I’d recycle his face</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgzSMwb88ntjkYHQYaN4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2019-06-19T04:00:06Z</td>\n",
       "      <td>i'd recycle his face</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>God dang you are a twat</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgwS5xQLLnyIzNUS4bp4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2019-06-16T00:49:53Z</td>\n",
       "      <td>god dang are a twat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Why didn't you give it away to one of the 3 fa...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Ugz1EJ6K2F0CYUX5NrN4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2019-03-31T00:50:58Z</td>\n",
       "      <td>why not give it away to one of 3 fans that rec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I’m trash but your trash and stupid I’m unsubs...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgyI5hLjBfIN99uVWDt4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2019-03-16T03:50:36Z</td>\n",
       "      <td>i'm trash but your trash and stupid i'm unsubs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>You are crazy man kkk</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgwDghsKi_E8H3xYe-V4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2019-02-22T17:48:07Z</td>\n",
       "      <td>are crazy man kkk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I physically hate you</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Ugxo2RWpZiVb6i0dGO54AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2019-02-04T15:31:57Z</td>\n",
       "      <td>physically hate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>First video i see of you and already fake. Goo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgwkHi8FfFqozu7c4KJ4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2019-02-02T09:09:34Z</td>\n",
       "      <td>first video see of and already fake. good job</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>That is so messed up</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Ugy-UnErAg23Btoph3x4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2019-02-01T01:53:55Z</td>\n",
       "      <td>that so messed up</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  positive  negative  \\\n",
       "0  *stretched past the 10 minute mark for ad reve...         0         1   \n",
       "1                                      Big time scam         0         1   \n",
       "2                               I’d recycle his face         0         1   \n",
       "3                            God dang you are a twat         0         1   \n",
       "4  Why didn't you give it away to one of the 3 fa...         0         1   \n",
       "5  I’m trash but your trash and stupid I’m unsubs...         0         1   \n",
       "6                              You are crazy man kkk         0         1   \n",
       "7                              I physically hate you         0         1   \n",
       "8  First video i see of you and already fake. Goo...         0         1   \n",
       "9                               That is so messed up         0         1   \n",
       "\n",
       "   neutral  rated                  comment_id     video_id  \\\n",
       "0        0      1  UgztP4lVR-Epv5HlSXN4AaABAg  ItYOdWRo0JY   \n",
       "1        0      1  UgzZubyLG5FtZu7qlal4AaABAg  ItYOdWRo0JY   \n",
       "2        0      1  UgzSMwb88ntjkYHQYaN4AaABAg  ItYOdWRo0JY   \n",
       "3        0      1  UgwS5xQLLnyIzNUS4bp4AaABAg  ItYOdWRo0JY   \n",
       "4        0      1  Ugz1EJ6K2F0CYUX5NrN4AaABAg  ItYOdWRo0JY   \n",
       "5        0      1  UgyI5hLjBfIN99uVWDt4AaABAg  ItYOdWRo0JY   \n",
       "6        0      1  UgwDghsKi_E8H3xYe-V4AaABAg  ItYOdWRo0JY   \n",
       "7        0      1  Ugxo2RWpZiVb6i0dGO54AaABAg  ItYOdWRo0JY   \n",
       "8        0      1  UgwkHi8FfFqozu7c4KJ4AaABAg  ItYOdWRo0JY   \n",
       "9        0      1  Ugy-UnErAg23Btoph3x4AaABAg  ItYOdWRo0JY   \n",
       "\n",
       "                   date                                            preproc  \n",
       "0  2020-01-10T20:24:33Z  *stretched past 10 minute mark for ad revenue,...  \n",
       "1  2019-08-01T15:45:49Z                                      big time scam  \n",
       "2  2019-06-19T04:00:06Z                               i'd recycle his face  \n",
       "3  2019-06-16T00:49:53Z                                god dang are a twat  \n",
       "4  2019-03-31T00:50:58Z  why not give it away to one of 3 fans that rec...  \n",
       "5  2019-03-16T03:50:36Z  i'm trash but your trash and stupid i'm unsubs...  \n",
       "6  2019-02-22T17:48:07Z                                  are crazy man kkk  \n",
       "7  2019-02-04T15:31:57Z                                    physically hate  \n",
       "8  2019-02-02T09:09:34Z      first video see of and already fake. good job  \n",
       "9  2019-02-01T01:53:55Z                                  that so messed up  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of emojis defined from https://en.wikipedia.org/wiki/List_of_emoticons , https://emojipedia.org/people/\n",
    "\n",
    "Other references:\n",
    "* https://www.urbandictionary.com/define.php?term=%F0%9F%92%80\n",
    "* https://www.urbandictionary.com/define.php?term=%F0%9F%94%A5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "# list of tagged emoticons from above links\n",
    "from acronyms_smileys import smileys\n",
    "from acronyms_smileys import sent_acronyms\n",
    "# for removing untagged emoji\n",
    "import demoji\n",
    "#demoji.download_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_repeating_emoji(text):\n",
    "    uniques = set()\n",
    "    final_string = list()\n",
    "    text_arr = [item for item in emoji.get_emoji_regexp().split(text) if not item == '']\n",
    "    for e in text_arr:\n",
    "        if not bool(emoji.get_emoji_regexp().search(e)):\n",
    "            final_string.append(smileys.get(e, e))\n",
    "        else:\n",
    "            if e not in uniques:\n",
    "                uniques.add(e)\n",
    "                final_string.append(smileys.get(e, e))\n",
    "    return ' '.join(final_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find repeating emoticons and remove repetitions, and tag emoticons\n",
    "df.preproc = df.preproc.apply(lambda x: replace_repeating_emoji(x) if (bool(emoji.get_emoji_regexp().search(x)) and bool(re.search(r'(.)\\1', x))) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove untagged emoticons\n",
    "df.preproc = df.preproc.apply(lambda x : demoji.replace(x, ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set sentiment on acronyms (such as 'lol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.preproc = df.preproc.apply(lambda x: ' '.join(sent_acronyms.get(word, word) for word in x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.preproc = df.preproc.apply(lambda x: ' '.join([word for word in x.split() if '#' not in word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>rated</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>date</th>\n",
       "      <th>preproc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [body, positive, negative, neutral, rated, comment_id, video_id, date, preproc]\n",
       "Index: []"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.preproc.str.contains('#')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "df.preproc = df.preproc.str.replace('[{}]'.format(string.punctuation), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>rated</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>date</th>\n",
       "      <th>preproc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1328</th>\n",
       "      <td>Oh! .. that was such a bad boo you fake news s...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgyyBBhr7F04uU5TkKh4AaABAg</td>\n",
       "      <td>FmE8DwZ6RHk</td>\n",
       "      <td>2018-02-01T08:07:41Z</td>\n",
       "      <td>oh  that was such a bad boo fake news station</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1329</th>\n",
       "      <td>you sound more racist than pewds, u fuckin leech.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgwDItRoRWNWE3Me4ZF4AaABAg</td>\n",
       "      <td>_UZbgH3pOBA</td>\n",
       "      <td>2017-11-11T03:37:51Z</td>\n",
       "      <td>sound more racist than pewds fuckin leech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330</th>\n",
       "      <td>Donald is right. Shut up cnn.</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgxY_W2PQVusRKHAuPl4AaABAg</td>\n",
       "      <td>FmE8DwZ6RHk</td>\n",
       "      <td>2018-01-28T15:05:38Z</td>\n",
       "      <td>donald right shut up cnn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1331</th>\n",
       "      <td>IM sorry you can't take care of a dog you want...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgwZBfZlFUxofuQpKBB4AaABAg</td>\n",
       "      <td>HVjlcUtuENM</td>\n",
       "      <td>2018-05-03T10:45:50Z</td>\n",
       "      <td>im sorry cant take care of a dog want a baby n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1332</th>\n",
       "      <td>Biased as fuck</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgjnBid2wZzZn3gCoAEC</td>\n",
       "      <td>cRJjXCK6H6Y</td>\n",
       "      <td>2016-09-03T13:05:10Z</td>\n",
       "      <td>biased as fuck</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   body  positive  negative  \\\n",
       "1328  Oh! .. that was such a bad boo you fake news s...         0         1   \n",
       "1329  you sound more racist than pewds, u fuckin leech.         0         1   \n",
       "1330                      Donald is right. Shut up cnn.         0         1   \n",
       "1331  IM sorry you can't take care of a dog you want...         0         1   \n",
       "1332                                     Biased as fuck         0         1   \n",
       "\n",
       "      neutral  rated                  comment_id     video_id  \\\n",
       "1328        0      1  UgyyBBhr7F04uU5TkKh4AaABAg  FmE8DwZ6RHk   \n",
       "1329        0      1  UgwDItRoRWNWE3Me4ZF4AaABAg  _UZbgH3pOBA   \n",
       "1330        0      1  UgxY_W2PQVusRKHAuPl4AaABAg  FmE8DwZ6RHk   \n",
       "1331        0      1  UgwZBfZlFUxofuQpKBB4AaABAg  HVjlcUtuENM   \n",
       "1332        0      1        UgjnBid2wZzZn3gCoAEC  cRJjXCK6H6Y   \n",
       "\n",
       "                      date                                            preproc  \n",
       "1328  2018-02-01T08:07:41Z      oh  that was such a bad boo fake news station  \n",
       "1329  2017-11-11T03:37:51Z          sound more racist than pewds fuckin leech  \n",
       "1330  2018-01-28T15:05:38Z                           donald right shut up cnn  \n",
       "1331  2018-05-03T10:45:50Z  im sorry cant take care of a dog want a baby n...  \n",
       "1332  2016-09-03T13:05:10Z                                     biased as fuck  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove repeating vowels and consonants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/46701245/how-to-replace-multiple-consecutive-repeating-characters-into-1-character-in-pyt\n",
    "df.preproc = df.preproc.apply(lambda x: ' '.join([re.sub(r'[^\\w\\s]|(.)(?=\\1)', '', word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tag any sequence of \"ha\" or \"ah\" (for example, \"ahaha\" or \"haha\") as a \"laugh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.preproc = df.preproc.apply(lambda x: ' '.join([re.sub(r'([ha]+[ah]+).*\\1', r'laugh', word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stem the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "snows = nltk.stem.SnowballStemmer('english')\n",
    "df.preproc = df.preproc.apply(lambda x: ' '.join([snows.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.preproc = df.preproc.apply(lambda row: nltk.word_tokenize(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>rated</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>date</th>\n",
       "      <th>preproc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>*stretched past the 10 minute mark for ad reve...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgztP4lVR-Epv5HlSXN4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2020-01-10T20:24:33Z</td>\n",
       "      <td>[stretch, past, 10, minut, mark, for, ad, reve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Big time scam</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgzZubyLG5FtZu7qlal4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2019-08-01T15:45:49Z</td>\n",
       "      <td>[big, time, scam]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I’d recycle his face</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgzSMwb88ntjkYHQYaN4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2019-06-19T04:00:06Z</td>\n",
       "      <td>[id, recycl, his, face]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>God dang you are a twat</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgwS5xQLLnyIzNUS4bp4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2019-06-16T00:49:53Z</td>\n",
       "      <td>[god, dang, are, a, twat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Why didn't you give it away to one of the 3 fa...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Ugz1EJ6K2F0CYUX5NrN4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2019-03-31T00:50:58Z</td>\n",
       "      <td>[whi, not, give, it, away, to, one, of, 3, fan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  positive  negative  \\\n",
       "0  *stretched past the 10 minute mark for ad reve...         0         1   \n",
       "1                                      Big time scam         0         1   \n",
       "2                               I’d recycle his face         0         1   \n",
       "3                            God dang you are a twat         0         1   \n",
       "4  Why didn't you give it away to one of the 3 fa...         0         1   \n",
       "\n",
       "   neutral  rated                  comment_id     video_id  \\\n",
       "0        0      1  UgztP4lVR-Epv5HlSXN4AaABAg  ItYOdWRo0JY   \n",
       "1        0      1  UgzZubyLG5FtZu7qlal4AaABAg  ItYOdWRo0JY   \n",
       "2        0      1  UgzSMwb88ntjkYHQYaN4AaABAg  ItYOdWRo0JY   \n",
       "3        0      1  UgwS5xQLLnyIzNUS4bp4AaABAg  ItYOdWRo0JY   \n",
       "4        0      1  Ugz1EJ6K2F0CYUX5NrN4AaABAg  ItYOdWRo0JY   \n",
       "\n",
       "                   date                                            preproc  \n",
       "0  2020-01-10T20:24:33Z  [stretch, past, 10, minut, mark, for, ad, reve...  \n",
       "1  2019-08-01T15:45:49Z                                  [big, time, scam]  \n",
       "2  2019-06-19T04:00:06Z                            [id, recycl, his, face]  \n",
       "3  2019-06-16T00:49:53Z                          [god, dang, are, a, twat]  \n",
       "4  2019-03-31T00:50:58Z  [whi, not, give, it, away, to, one, of, 3, fan...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix:\n",
    "* if spell checking is not performed, make sure that for negations misspelled words are considered, like 'cant' and so on.\n",
    "\n",
    "To do:\n",
    "* profanity tags\n",
    "* remove numbers\n",
    "\n",
    "after that\n",
    "* tf-idf for feature extraction? look into dictionaries: hownet, ntusd\n",
    "* word2vec for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
