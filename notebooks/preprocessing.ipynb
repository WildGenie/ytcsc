{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import enchant\n",
    "from enchant.checker import SpellChecker\n",
    "import enchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version\n",
      "3.7.7 (tags/v3.7.7:d7c567b08f, Mar 10 2020, 10:41:24) [MSC v.1900 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python version\")\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/raw_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the text more uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.body = df.body.replace(r'http\\S+', '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change acronyms to words for uniformity. \n",
    "* https://www.netlingo.com/acronyms.php\n",
    "* https://blog.adioma.com/internet-acronyms-intro-list-infographic/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# internet slangs\n",
    "acronyms = {\n",
    "    'btw': 'by the way',\n",
    "    'afaik': 'as far as i know',\n",
    "    'afk': 'away from keyboard',\n",
    "    'asap': 'as soon as possible',\n",
    "    'atm': 'at the moment',\n",
    "    'bbl': 'be back later',\n",
    "    'brb': 'be right back',\n",
    "    'b4': 'before',\n",
    "    'cya': 'see ya',\n",
    "    'dae': 'does anyone else',\n",
    "    'eli5': 'explain like i\\'m five',\n",
    "    'fyi': 'for your information',\n",
    "    'faq': 'frequently asked questions',\n",
    "    'gg': 'good game',\n",
    "    'gr8': 'great',\n",
    "    'imo': 'in my opinion',\n",
    "    'imho': 'in my honest opinion',\n",
    "    'iirc': 'if i remember correctly',\n",
    "    'irl': 'in real life',\n",
    "    'l8r': 'later',\n",
    "    'm8': 'mate',\n",
    "    'mrw': 'my reaction when',\n",
    "    'mfw': 'my face when',\n",
    "    'pita': 'pain in the ass',\n",
    "    'thx': 'thanks',\n",
    "    'til': 'today i learned',\n",
    "    'tbh': 'to be honest',\n",
    "    'u': 'you',\n",
    "    'y': 'why',\n",
    "    'r': 'are',\n",
    "    'ur': 'your',\n",
    "    'w8': 'wait',\n",
    "    '2nite': 'tonight',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spell checker with uppercase some letters later on\n",
    "df.body = df.body.str.lower()\n",
    "# the ’ was causing issues, took a while to notice\n",
    "df.body = df.body.str.replace('’', '\\'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.body = df.body.apply(lambda x: ' '.join(acronyms.get(word, word) for word in x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix as many spelling errors as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell_checker = SpellChecker(\"en_UK\",\"en_US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_error(body):\n",
    "    spell_checker.set_text(body)\n",
    "    for err in spell_checker:\n",
    "        if len(err.suggest())>0: \n",
    "            sug = err.suggest()[0]\n",
    "            err.replace(sug)\n",
    "    return spell_checker.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.body = df.body.apply(lambda row: correct_error(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stop words and replace negations with \"not\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again, thanks to spell checker\n",
    "df.body = df.body.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english')\n",
    "negations = ['don\\'t', 'aint' 'aren\\'t', 'couldn\\'t','didn\\'t', \n",
    "             'doesn\\'t', 'hadn\\'t', 'hasn\\'t', 'haven\\'t', 'isn\\'t', \n",
    "             'mightn\\'t', 'mustn\\'t', 'needn\\'t', 'shouldn\\'t', 'wasn\\'t', \n",
    "             'weren\\'t', 'won\\'t', 'wouldn\\'t', 'nor', 'not']\n",
    "stopwords_list = [el for el in stopwords_list if el not in negations]\n",
    "missing_words = ['i\\'m', 'i\\'d']\n",
    "stopwords_list.extend(missing_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "regx = r'\\b(?:{})\\b'.format('|'.join(negations))\n",
    "df.body = df.body.str.replace(regx, 'not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.body = df.body.apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords_list)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>rated</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>*stretched past 10 minute mark ad revenue, see!*</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgztP4lVR-Epv5HlSXN4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2020-01-10T20:24:33Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>big time scam</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgzZubyLG5FtZu7qlal4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2019-08-01T15:45:49Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recycle face</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgzSMwb88ntjkYHQYaN4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2019-06-19T04:00:06Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>god dang twat</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgwS5xQLLnyIzNUS4bp4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2019-06-16T00:49:53Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>not give away one 3 fans recognized you?</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Ugz1EJ6K2F0CYUX5NrN4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2019-03-31T00:50:58Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>trash trash stupid unsubscribing</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgyI5hLjBfIN99uVWDt4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2019-03-16T03:50:36Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>crazy man kkk</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgwDghsKi_E8H3xYe-V4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2019-02-22T17:48:07Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>physically hate</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Ugxo2RWpZiVb6i0dGO54AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2019-02-04T15:31:57Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>first video see already fake. good job</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgwkHi8FfFqozu7c4KJ4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2019-02-02T09:09:34Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>messed</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Ugy-UnErAg23Btoph3x4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2019-02-01T01:53:55Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               body  positive  negative  \\\n",
       "0  *stretched past 10 minute mark ad revenue, see!*         0         1   \n",
       "1                                     big time scam         0         1   \n",
       "2                                      recycle face         0         1   \n",
       "3                                     god dang twat         0         1   \n",
       "4          not give away one 3 fans recognized you?         0         1   \n",
       "5                  trash trash stupid unsubscribing         0         1   \n",
       "6                                     crazy man kkk         0         1   \n",
       "7                                   physically hate         0         1   \n",
       "8            first video see already fake. good job         0         1   \n",
       "9                                            messed         0         1   \n",
       "\n",
       "   neutral  rated                  comment_id     video_id  \\\n",
       "0        0      1  UgztP4lVR-Epv5HlSXN4AaABAg  ItYOdWRo0JY   \n",
       "1        0      1  UgzZubyLG5FtZu7qlal4AaABAg  ItYOdWRo0JY   \n",
       "2        0      1  UgzSMwb88ntjkYHQYaN4AaABAg  ItYOdWRo0JY   \n",
       "3        0      1  UgwS5xQLLnyIzNUS4bp4AaABAg  ItYOdWRo0JY   \n",
       "4        0      1  Ugz1EJ6K2F0CYUX5NrN4AaABAg  ItYOdWRo0JY   \n",
       "5        0      1  UgyI5hLjBfIN99uVWDt4AaABAg  ItYOdWRo0JY   \n",
       "6        0      1  UgwDghsKi_E8H3xYe-V4AaABAg  ItYOdWRo0JY   \n",
       "7        0      1  Ugxo2RWpZiVb6i0dGO54AaABAg  ItYOdWRo0JY   \n",
       "8        0      1  UgwkHi8FfFqozu7c4KJ4AaABAg  ItYOdWRo0JY   \n",
       "9        0      1  Ugy-UnErAg23Btoph3x4AaABAg  ItYOdWRo0JY   \n",
       "\n",
       "                   date  \n",
       "0  2020-01-10T20:24:33Z  \n",
       "1  2019-08-01T15:45:49Z  \n",
       "2  2019-06-19T04:00:06Z  \n",
       "3  2019-06-16T00:49:53Z  \n",
       "4  2019-03-31T00:50:58Z  \n",
       "5  2019-03-16T03:50:36Z  \n",
       "6  2019-02-22T17:48:07Z  \n",
       "7  2019-02-04T15:31:57Z  \n",
       "8  2019-02-02T09:09:34Z  \n",
       "9  2019-02-01T01:53:55Z  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of emojis defined from https://en.wikipedia.org/wiki/List_of_emoticons , https://emojipedia.org/people/\n",
    "\n",
    "Other references:\n",
    "* https://www.urbandictionary.com/define.php?term=%F0%9F%92%80\n",
    "* https://www.urbandictionary.com/define.php?term=%F0%9F%94%A5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "# list of tagged emoticons from above links\n",
    "from smileys import smileys\n",
    "from smileys import sent_acronyms\n",
    "# for removing untagged emoji\n",
    "import demoji\n",
    "#demoji.download_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_repeating_emoji(text):\n",
    "    uniques = set()\n",
    "    final_string = list()\n",
    "    text_arr = [item for item in emoji.get_emoji_regexp().split(text) if not item == '']\n",
    "    for e in text_arr:\n",
    "        if not bool(emoji.get_emoji_regexp().search(e)):\n",
    "            final_string.append(smileys.get(e, e))\n",
    "        else:\n",
    "            if e not in uniques:\n",
    "                uniques.add(e)\n",
    "                final_string.append(smileys.get(e, e))\n",
    "    return ' '.join(final_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find repeating emoticons and remove repetitions, and tag emoticons\n",
    "df.body = df.body.apply(lambda x: replace_repeating_emoji(x) if (bool(emoji.get_emoji_regexp().search(x)) and bool(re.search(r'(.)\\1', x))) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove untagged emoticons\n",
    "df.body = df.body.apply(lambda x : demoji.replace(x, ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set sentiment on acronyms (such as 'lol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.body = df.body.apply(lambda x: ' '.join(sent_acronyms.get(word, word) for word in x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.body = df.body.apply(lambda x: ' '.join([word for word in x.split() if '#' not in word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>rated</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [body, positive, negative, neutral, rated, comment_id, video_id, date]\n",
       "Index: []"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.body.str.contains('#')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "df.body = df.body.str.replace('[{}]'.format(string.punctuation), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>rated</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1323</th>\n",
       "      <td>many people believe</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgiH2C_pE_zFgHgCoAEC</td>\n",
       "      <td>cRJjXCK6H6Y</td>\n",
       "      <td>2016-08-21T12:48:52Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1324</th>\n",
       "      <td>cant believe d sorry currently not access feat...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Ugxl39L-FybtONur09J4AaABAg</td>\n",
       "      <td>8d_202l55LU</td>\n",
       "      <td>2017-12-16T05:41:20Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1325</th>\n",
       "      <td>honestly one better videos ive seen days gave ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgyyZgg2kULZsLwTMjF4AaABAg</td>\n",
       "      <td>YbJOTdZBX1g</td>\n",
       "      <td>2021-02-01T19:21:37Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1326</th>\n",
       "      <td>feel stabbed back leafy not finish like hoped</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgiveoyqvwtYdXgCoAEC</td>\n",
       "      <td>Rqy_5PmID6k</td>\n",
       "      <td>2016-06-22T20:45:17Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1327</th>\n",
       "      <td>mom</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgyTfLvliRAjqdG83jd4AaABAg</td>\n",
       "      <td>udBgnu1hbLU</td>\n",
       "      <td>2020-01-03T22:09:58Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1328</th>\n",
       "      <td>oh  bad boo fake news station</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgyyBBhr7F04uU5TkKh4AaABAg</td>\n",
       "      <td>FmE8DwZ6RHk</td>\n",
       "      <td>2018-02-01T08:07:41Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1329</th>\n",
       "      <td>sound racist pewds fuckin leech</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgwDItRoRWNWE3Me4ZF4AaABAg</td>\n",
       "      <td>_UZbgH3pOBA</td>\n",
       "      <td>2017-11-11T03:37:51Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1330</th>\n",
       "      <td>donald right shut cnn</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgxY_W2PQVusRKHAuPl4AaABAg</td>\n",
       "      <td>FmE8DwZ6RHk</td>\n",
       "      <td>2018-01-28T15:05:38Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1331</th>\n",
       "      <td>im sorry cant take care dog want baby  please</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgwZBfZlFUxofuQpKBB4AaABAg</td>\n",
       "      <td>HVjlcUtuENM</td>\n",
       "      <td>2018-05-03T10:45:50Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1332</th>\n",
       "      <td>biased fuck</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgjnBid2wZzZn3gCoAEC</td>\n",
       "      <td>cRJjXCK6H6Y</td>\n",
       "      <td>2016-09-03T13:05:10Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   body  positive  negative  \\\n",
       "1323                                many people believe         0         1   \n",
       "1324  cant believe d sorry currently not access feat...         0         1   \n",
       "1325  honestly one better videos ive seen days gave ...         1         0   \n",
       "1326      feel stabbed back leafy not finish like hoped         0         1   \n",
       "1327                                                mom         0         1   \n",
       "1328                      oh  bad boo fake news station         0         1   \n",
       "1329                    sound racist pewds fuckin leech         0         1   \n",
       "1330                              donald right shut cnn         0         1   \n",
       "1331      im sorry cant take care dog want baby  please         0         1   \n",
       "1332                                        biased fuck         0         1   \n",
       "\n",
       "      neutral  rated                  comment_id     video_id  \\\n",
       "1323        0      1        UgiH2C_pE_zFgHgCoAEC  cRJjXCK6H6Y   \n",
       "1324        0      1  Ugxl39L-FybtONur09J4AaABAg  8d_202l55LU   \n",
       "1325        0      1  UgyyZgg2kULZsLwTMjF4AaABAg  YbJOTdZBX1g   \n",
       "1326        0      1        UgiveoyqvwtYdXgCoAEC  Rqy_5PmID6k   \n",
       "1327        0      1  UgyTfLvliRAjqdG83jd4AaABAg  udBgnu1hbLU   \n",
       "1328        0      1  UgyyBBhr7F04uU5TkKh4AaABAg  FmE8DwZ6RHk   \n",
       "1329        0      1  UgwDItRoRWNWE3Me4ZF4AaABAg  _UZbgH3pOBA   \n",
       "1330        0      1  UgxY_W2PQVusRKHAuPl4AaABAg  FmE8DwZ6RHk   \n",
       "1331        0      1  UgwZBfZlFUxofuQpKBB4AaABAg  HVjlcUtuENM   \n",
       "1332        0      1        UgjnBid2wZzZn3gCoAEC  cRJjXCK6H6Y   \n",
       "\n",
       "                      date  \n",
       "1323  2016-08-21T12:48:52Z  \n",
       "1324  2017-12-16T05:41:20Z  \n",
       "1325  2021-02-01T19:21:37Z  \n",
       "1326  2016-06-22T20:45:17Z  \n",
       "1327  2020-01-03T22:09:58Z  \n",
       "1328  2018-02-01T08:07:41Z  \n",
       "1329  2017-11-11T03:37:51Z  \n",
       "1330  2018-01-28T15:05:38Z  \n",
       "1331  2018-05-03T10:45:50Z  \n",
       "1332  2016-09-03T13:05:10Z  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove repeating vowels and consonants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/46701245/how-to-replace-multiple-consecutive-repeating-characters-into-1-character-in-pyt\n",
    "df.body = df.body.apply(lambda x: ' '.join([re.sub(r'[^\\w\\s]|(.)(?=\\1)', '', word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tag any sequence of \"ha\" or \"ah\" (for example, \"ahaha\" or \"haha\") as a \"laugh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.body = df.body.apply(lambda x: ' '.join([re.sub(r'([ha]+[ah]+).*\\1', r'laugh', word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To fix:\n",
    "* if spell checking is not performed, make sure that for negations misspelled words are considered, like 'cant' and so on.\n",
    "\n",
    "To do:\n",
    "* stemming\n",
    "* consider replacing insults into a tag, like \"bad_word\"\n",
    "* tokenize\n",
    "\n",
    "after that\n",
    "* tf-idf for feature extraction? look into dictionaries: hownet, ntusd\n",
    "* word2vec for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
