{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import enchant\n",
    "from enchant.checker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/raw_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making text more uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.body = df.body.replace(r'http\\S+', '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix as many spelling errors as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell_checker = SpellChecker(\"en_UK\",\"en_US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the ’ was causing issues, took a while to notice\n",
    "df.body = df.body.str.replace('’', '\\'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_error(body):\n",
    "    spell_checker.set_text(body)\n",
    "    for err in spell_checker:\n",
    "        if len(err.suggest())>0: \n",
    "            sug = err.suggest()[0]\n",
    "            err.replace(sug)\n",
    "    return spell_checker.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.body = df.body.apply(lambda row: correct_error(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stop words and replace negations with \"not\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.body = df.body.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english')\n",
    "negations = ['don\\'t', 'aint' 'aren\\'t', 'couldn\\'t','didn\\'t', \n",
    "             'doesn\\'t', 'hadn\\'t', 'hasn\\'t', 'haven\\'t', 'isn\\'t', \n",
    "             'mightn\\'t', 'mustn\\'t', 'needn\\'t', 'shouldn\\'t', 'wasn\\'t', \n",
    "             'weren\\'t', 'won\\'t', 'wouldn\\'t', 'nor', 'not']\n",
    "stopwords_list = [el for el in stopwords_list if el not in negations]\n",
    "missing_words = ['i\\'m', 'i\\'d']\n",
    "stopwords_list.extend(missing_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "regx = r'\\b(?:{})\\b'.format('|'.join(negations))\n",
    "df.body = df.body.str.replace(regx, 'not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.body = df.body.apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords_list)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>rated</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>*stretched past 10 minute mark ad revenue, see!*</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgztP4lVR-Epv5HlSXN4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2020-01-10T20:24:33Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>big time scam</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgzZubyLG5FtZu7qlal4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2019-08-01T15:45:49Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>recycle face</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgzSMwb88ntjkYHQYaN4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2019-06-19T04:00:06Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>god dang twat</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgwS5xQLLnyIzNUS4bp4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2019-06-16T00:49:53Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>not give away one 3 fans recognized you?</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Ugz1EJ6K2F0CYUX5NrN4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2019-03-31T00:50:58Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>trash trash stupid unsubscribing</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgyI5hLjBfIN99uVWDt4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2019-03-16T03:50:36Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>crazy man kkk</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgwDghsKi_E8H3xYe-V4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2019-02-22T17:48:07Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>physically hate</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Ugxo2RWpZiVb6i0dGO54AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2019-02-04T15:31:57Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>first video see already fake. good job</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgwkHi8FfFqozu7c4KJ4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2019-02-02T09:09:34Z</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>messed</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Ugy-UnErAg23Btoph3x4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2019-02-01T01:53:55Z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               body  positive  negative  \\\n",
       "0  *stretched past 10 minute mark ad revenue, see!*         0         1   \n",
       "1                                     big time scam         0         1   \n",
       "2                                      recycle face         0         1   \n",
       "3                                     god dang twat         0         1   \n",
       "4          not give away one 3 fans recognized you?         0         1   \n",
       "5                  trash trash stupid unsubscribing         0         1   \n",
       "6                                     crazy man kkk         0         1   \n",
       "7                                   physically hate         0         1   \n",
       "8            first video see already fake. good job         0         1   \n",
       "9                                            messed         0         1   \n",
       "\n",
       "   neutral  rated                  comment_id     video_id  \\\n",
       "0        0      1  UgztP4lVR-Epv5HlSXN4AaABAg  ItYOdWRo0JY   \n",
       "1        0      1  UgzZubyLG5FtZu7qlal4AaABAg  ItYOdWRo0JY   \n",
       "2        0      1  UgzSMwb88ntjkYHQYaN4AaABAg  ItYOdWRo0JY   \n",
       "3        0      1  UgwS5xQLLnyIzNUS4bp4AaABAg  ItYOdWRo0JY   \n",
       "4        0      1  Ugz1EJ6K2F0CYUX5NrN4AaABAg  ItYOdWRo0JY   \n",
       "5        0      1  UgyI5hLjBfIN99uVWDt4AaABAg  ItYOdWRo0JY   \n",
       "6        0      1  UgwDghsKi_E8H3xYe-V4AaABAg  ItYOdWRo0JY   \n",
       "7        0      1  Ugxo2RWpZiVb6i0dGO54AaABAg  ItYOdWRo0JY   \n",
       "8        0      1  UgwkHi8FfFqozu7c4KJ4AaABAg  ItYOdWRo0JY   \n",
       "9        0      1  Ugy-UnErAg23Btoph3x4AaABAg  ItYOdWRo0JY   \n",
       "\n",
       "                   date  \n",
       "0  2020-01-10T20:24:33Z  \n",
       "1  2019-08-01T15:45:49Z  \n",
       "2  2019-06-19T04:00:06Z  \n",
       "3  2019-06-16T00:49:53Z  \n",
       "4  2019-03-31T00:50:58Z  \n",
       "5  2019-03-16T03:50:36Z  \n",
       "6  2019-02-22T17:48:07Z  \n",
       "7  2019-02-04T15:31:57Z  \n",
       "8  2019-02-02T09:09:34Z  \n",
       "9  2019-02-01T01:53:55Z  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* replace emojis with their tags\n",
    "* replace smileys ( \":)\" etc.)\n",
    "* consider if removing repeating consonants is a good idea\n",
    "* remove punctuation\n",
    "* find \"haha\" and replace with \"laugh\"\n",
    "* stemming\n",
    "* consider replacing insults into a tag, like \"bad_word\"\n",
    "* tokenize\n",
    "\n",
    "after that\n",
    "* tf-idf for feature extraction? look into dictionaries: hownet, ntusd\n",
    "* word2vec for embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
