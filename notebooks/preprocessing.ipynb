{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "import enchant\n",
    "from enchant.checker import SpellChecker\n",
    "import enchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version\n",
      "3.7.7 (tags/v3.7.7:d7c567b08f, Mar 10 2020, 10:41:24) [MSC v.1900 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"Python version\")\n",
    "print (sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/raw_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the text more uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preproc'] = df.body.replace(r'http\\S+', '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change acronyms to words for uniformity. \n",
    "* https://www.netlingo.com/acronyms.php\n",
    "* https://blog.adioma.com/internet-acronyms-intro-list-infographic/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from acronyms_smileys import acronyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.preproc = df.preproc.str.lower()\n",
    "# the ’ was causing issues, took a while to notice\n",
    "df.preproc = df.preproc.str.replace('’', '\\'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.preproc = df.preproc.apply(lambda x: ' '.join(acronyms.get(word, word) for word in x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace negations with \"not\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "negations = ['don\\'t', 'aint' 'aren\\'t', 'couldn\\'t','didn\\'t', \n",
    "             'doesn\\'t', 'hadn\\'t', 'hasn\\'t', 'haven\\'t', 'isn\\'t', \n",
    "             'mightn\\'t', 'mustn\\'t', 'needn\\'t', 'shouldn\\'t', 'wasn\\'t', \n",
    "             'weren\\'t', 'won\\'t', 'wouldn\\'t', 'nor', 'not', 'cant', 'dont',\n",
    "            'arent', 'couldnt', 'didnt', 'doesnt', 'hadnt', 'hasnt', 'havent',\n",
    "            'isnt', 'mightnt', 'mustnt', 'neednt', 'shouldnt', 'wasnt',\n",
    "            'werent', 'wont', 'wouldnt']\n",
    "regx = r'\\b(?:{})\\b'.format('|'.join(negations))\n",
    "df.preproc = df.preproc.str.replace(regx, 'not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove negations from stop list, add two missing contractions\n",
    "stopwords_list = stopwords.words('english')\n",
    "stopwords_list = [el for el in stopwords_list if el not in negations]\n",
    "missing_words = ['i\\'m', 'i\\'d']\n",
    "stopwords_list.extend(missing_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 most commonly used words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "this    432\n",
       "the     426\n",
       "i       373\n",
       "is      302\n",
       "you     277\n",
       "to      263\n",
       "a       241\n",
       "not     197\n",
       "and     185\n",
       "of      172\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(' '.join(df.preproc).lower().split()).value_counts()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common_stopwords(stop_words, n=5):\n",
    "    most_freq_words = pd.Series(' '.join(df.preproc).lower().split()).value_counts()[:int(n*2)].keys().to_numpy()\n",
    "    common_stopwords = [i for i in most_freq_words if i in stop_words]\n",
    "    return common_stopwords[0:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove most common stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_stopwords = get_common_stopwords(stopwords_list)\n",
    "df.preproc = df.preproc.apply(lambda x: ' '.join([word for word in x.split() if word not in (common_stopwords)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>rated</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>date</th>\n",
       "      <th>preproc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>*stretched past the 10 minute mark for ad reve...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgztP4lVR-Epv5HlSXN4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2020-01-10T20:24:33Z</td>\n",
       "      <td>*stretched past 10 minute mark for ad revenue,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Big time scam</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgzZubyLG5FtZu7qlal4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2019-08-01T15:45:49Z</td>\n",
       "      <td>big time scam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I’d recycle his face</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgzSMwb88ntjkYHQYaN4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2019-06-19T04:00:06Z</td>\n",
       "      <td>i'd recycle his face</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>God dang you are a twat</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgwS5xQLLnyIzNUS4bp4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2019-06-16T00:49:53Z</td>\n",
       "      <td>god dang are a twat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Why didn't you give it away to one of the 3 fa...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Ugz1EJ6K2F0CYUX5NrN4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2019-03-31T00:50:58Z</td>\n",
       "      <td>why not give it away to one of 3 fans that rec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  positive  negative  \\\n",
       "0  *stretched past the 10 minute mark for ad reve...         0         1   \n",
       "1                                      Big time scam         0         1   \n",
       "2                               I’d recycle his face         0         1   \n",
       "3                            God dang you are a twat         0         1   \n",
       "4  Why didn't you give it away to one of the 3 fa...         0         1   \n",
       "\n",
       "   neutral  rated                  comment_id     video_id  \\\n",
       "0        0      1  UgztP4lVR-Epv5HlSXN4AaABAg  ItYOdWRo0JY   \n",
       "1        0      1  UgzZubyLG5FtZu7qlal4AaABAg  ItYOdWRo0JY   \n",
       "2        0      1  UgzSMwb88ntjkYHQYaN4AaABAg  ItYOdWRo0JY   \n",
       "3        0      1  UgwS5xQLLnyIzNUS4bp4AaABAg  ItYOdWRo0JY   \n",
       "4        0      1  Ugz1EJ6K2F0CYUX5NrN4AaABAg  ItYOdWRo0JY   \n",
       "\n",
       "                   date                                            preproc  \n",
       "0  2020-01-10T20:24:33Z  *stretched past 10 minute mark for ad revenue,...  \n",
       "1  2019-08-01T15:45:49Z                                      big time scam  \n",
       "2  2019-06-19T04:00:06Z                               i'd recycle his face  \n",
       "3  2019-06-16T00:49:53Z                                god dang are a twat  \n",
       "4  2019-03-31T00:50:58Z  why not give it away to one of 3 fans that rec...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of emojis defined from https://en.wikipedia.org/wiki/List_of_emoticons , https://emojipedia.org/people/\n",
    "\n",
    "Other references:\n",
    "* https://www.urbandictionary.com/define.php?term=%F0%9F%92%80\n",
    "* https://www.urbandictionary.com/define.php?term=%F0%9F%94%A5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "# list of tagged emoticons from above links\n",
    "from acronyms_smileys import smileys\n",
    "from acronyms_smileys import sent_acronyms\n",
    "# for removing untagged emoji\n",
    "import demoji\n",
    "#demoji.download_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_repeating_emoji(text):\n",
    "    uniques = set()\n",
    "    final_string = list()\n",
    "    text_arr = [item for item in emoji.get_emoji_regexp().split(text) if not item == '']\n",
    "    for e in text_arr:\n",
    "        # for some reason even though it is defined as '❤', when its \n",
    "        # imported, it gets loaded as '❤❤'\n",
    "        if e == '❤':\n",
    "            e = '❤❤'\n",
    "        if not bool(emoji.get_emoji_regexp().search(e)):\n",
    "            final_string.append(smileys.get(e, e))\n",
    "        else:\n",
    "            if e not in uniques:\n",
    "                uniques.add(e)\n",
    "                final_string.append(smileys.get(e, e))\n",
    "    return ' '.join(final_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find repeating emoticons and remove repetitions, and tag emoticons\n",
    "df.preproc = df.preproc.apply(lambda x: replace_repeating_emoji(x) if (bool(emoji.get_emoji_regexp().search(x)) and bool(re.search(r'(.)\\1', x))) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove untagged emoticons\n",
    "df.preproc = df.preproc.apply(lambda x : demoji.replace(x, ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set sentiment on acronyms (such as 'lol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.preproc = df.preproc.apply(lambda x: ' '.join(sent_acronyms.get(word, word) for word in x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.preproc = df.preproc.apply(lambda x: ' '.join([word for word in x.split() if '#' not in word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>rated</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>date</th>\n",
       "      <th>preproc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [body, positive, negative, neutral, rated, comment_id, video_id, date, preproc]\n",
       "Index: []"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df.preproc.str.contains('#')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "df.preproc = df.preproc.str.replace('[{}]'.format(string.punctuation), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>rated</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>video_id</th>\n",
       "      <th>date</th>\n",
       "      <th>preproc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>*stretched past the 10 minute mark for ad reve...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgztP4lVR-Epv5HlSXN4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2020-01-10T20:24:33Z</td>\n",
       "      <td>stretched past 10 minute mark for ad revenue see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Big time scam</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgzZubyLG5FtZu7qlal4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2019-08-01T15:45:49Z</td>\n",
       "      <td>big time scam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I’d recycle his face</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgzSMwb88ntjkYHQYaN4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2019-06-19T04:00:06Z</td>\n",
       "      <td>id recycle his face</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>God dang you are a twat</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>UgwS5xQLLnyIzNUS4bp4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2019-06-16T00:49:53Z</td>\n",
       "      <td>god dang are a twat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Why didn't you give it away to one of the 3 fa...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Ugz1EJ6K2F0CYUX5NrN4AaABAg</td>\n",
       "      <td>ItYOdWRo0JY</td>\n",
       "      <td>2019-03-31T00:50:58Z</td>\n",
       "      <td>why not give it away to one of 3 fans that rec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  positive  negative  \\\n",
       "0  *stretched past the 10 minute mark for ad reve...         0         1   \n",
       "1                                      Big time scam         0         1   \n",
       "2                               I’d recycle his face         0         1   \n",
       "3                            God dang you are a twat         0         1   \n",
       "4  Why didn't you give it away to one of the 3 fa...         0         1   \n",
       "\n",
       "   neutral  rated                  comment_id     video_id  \\\n",
       "0        0      1  UgztP4lVR-Epv5HlSXN4AaABAg  ItYOdWRo0JY   \n",
       "1        0      1  UgzZubyLG5FtZu7qlal4AaABAg  ItYOdWRo0JY   \n",
       "2        0      1  UgzSMwb88ntjkYHQYaN4AaABAg  ItYOdWRo0JY   \n",
       "3        0      1  UgwS5xQLLnyIzNUS4bp4AaABAg  ItYOdWRo0JY   \n",
       "4        0      1  Ugz1EJ6K2F0CYUX5NrN4AaABAg  ItYOdWRo0JY   \n",
       "\n",
       "                   date                                            preproc  \n",
       "0  2020-01-10T20:24:33Z   stretched past 10 minute mark for ad revenue see  \n",
       "1  2019-08-01T15:45:49Z                                      big time scam  \n",
       "2  2019-06-19T04:00:06Z                                id recycle his face  \n",
       "3  2019-06-16T00:49:53Z                                god dang are a twat  \n",
       "4  2019-03-31T00:50:58Z  why not give it away to one of 3 fans that rec...  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove repeating vowels and consonants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/46701245/how-to-replace-multiple-consecutive-repeating-characters-into-1-character-in-pyt\n",
    "df.preproc = df.preproc.apply(lambda x: ' '.join([re.sub(r'[^\\w\\s]|(.)(?=\\1)', '', word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tag any sequence of \"ha\" or \"ah\" (for example, \"ahaha\" or \"haha\") as a \"laugh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.preproc = df.preproc.apply(lambda x: ' '.join([re.sub(r'([ha]+[ah]+).*\\1', r'laugh', word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.preproc = df.preproc.str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 - negative, 1 - positive\n",
    "df['rating'] = df.positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write preprocessed column and the rating to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy relevant columns for later to file write\n",
    "data_no_trans_stem = df.filter(['comment_id', 'preproc', 'rating'], axis=1)\n",
    "data_no_trans_stem.columns = ['comment_id', 'body', 'rating']\n",
    "data_trans = df.filter(['comment_id', 'preproc', 'rating'], axis=1)\n",
    "data_trans.columns = ['comment_id', 'body', 'rating']\n",
    "data_stem = df.filter(['comment_id', 'preproc', 'rating'], axis=1)\n",
    "data_stem.columns = ['comment_id', 'body', 'rating']\n",
    "data_trans_stem = df.filter(['comment_id', 'preproc', 'rating'], axis=1)\n",
    "data_trans_stem.columns = ['comment_id', 'body', 'rating']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix as many spelling errors as possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell_checker = SpellChecker(\"en_UK\",\"en_US\")\n",
    "def correct_error(body):\n",
    "    spell_checker.set_text(body)\n",
    "    for err in spell_checker:\n",
    "        if len(err.suggest())>0: \n",
    "            sug = err.suggest()[0]\n",
    "            err.replace(sug)\n",
    "    return spell_checker.get_text()\n",
    "data_trans.body = data_trans.body.apply(lambda row: correct_error(row))\n",
    "data_trans_stem.body = data_trans_stem.body.apply(lambda row: correct_error(row))\n",
    "data_trans.body = data_trans.body.str.lower()\n",
    "data_trans_stem.body = data_trans_stem.body.str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stem the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "snows = nltk.stem.SnowballStemmer('english')\n",
    "data_stem.body = data_stem.body.apply(lambda x: ' '.join([snows.stem(word) for word in x.split()]))\n",
    "data_trans_stem.body = data_trans_stem.body.apply(lambda x: ' '.join([snows.stem(word) for word in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_trans_stem.to_csv('../data/preproc_no_trans_stem.csv', index=False)\n",
    "data_trans.to_csv('../data/preproc_trans.csv', index=False)\n",
    "data_stem.to_csv('../data/preproc_stem.csv', index=False)\n",
    "data_trans_stem.to_csv('../data/preproc_trans_stem.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do:\n",
    "* profanity tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
