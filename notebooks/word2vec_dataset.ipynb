{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pleased-smith",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "extreme-basic",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_trans_stem = pd.read_csv('../data/preproc_no_trans_stem.csv')\n",
    "stem = pd.read_csv('../data/preproc_stem.csv')\n",
    "long_cmts = pd.read_csv('../data/preproc_long.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subsequent-cornwall",
   "metadata": {},
   "source": [
    "Build w2v models and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "choice-scope",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_no_trans_stem = no_trans_stem.copy()\n",
    "w2v_stem = stem.copy()\n",
    "w2v_long = long_cmts.copy()\n",
    "sentences_w2v_no_trans_stem = [[word for word in str(body).split()] for body in w2v_no_trans_stem.body]\n",
    "sentences_w2v_stem = [[word for word in str(body).split()] for body in w2v_stem.body]\n",
    "sentences_w2v_long = [[word for word in str(body).split()] for body in w2v_long.body]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "super-medline",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "min_word_count = 1\n",
    "vector_size=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "sitting-trauma",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('her', 0.9999401569366455),\n",
       " ('song', 0.9999393224716187),\n",
       " ('selena', 0.9999318718910217),\n",
       " ('videos', 0.9999314546585083),\n",
       " ('amazing', 0.9999299049377441),\n",
       " ('beautiful', 0.9999279975891113),\n",
       " ('so', 0.9999265074729919),\n",
       " ('actualy', 0.9999256134033203),\n",
       " ('when', 0.9999215602874756),\n",
       " ('funy', 0.9999215006828308)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec = Word2Vec(sentences=sentences_w2v_no_trans_stem, seed=seed, min_count=1, size=vector_size)\n",
    "word2vec.wv.most_similar('love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "removable-today",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('song', 0.9999301433563232),\n",
       " ('amaz', 0.9999276399612427),\n",
       " ('god', 0.999923050403595),\n",
       " ('her', 0.9999221563339233),\n",
       " ('so', 0.9999215006828308),\n",
       " ('funi', 0.9999207258224487),\n",
       " ('much', 0.9999203085899353),\n",
       " ('selena', 0.999920129776001),\n",
       " ('beauti', 0.9999179840087891),\n",
       " ('great', 0.999914824962616)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_stem = Word2Vec(sentences=sentences_w2v_stem, seed=seed, min_count=1, size=vector_size)\n",
    "word2vec_stem.wv.most_similar('love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "decreased-phoenix",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('song', 0.9999653100967407),\n",
       " ('now', 0.9999630451202393),\n",
       " ('people', 0.9999624490737915),\n",
       " ('because', 0.9999600648880005),\n",
       " ('when', 0.9999589920043945),\n",
       " ('am', 0.9999585747718811),\n",
       " ('to', 0.9999572038650513),\n",
       " ('guy', 0.9999569654464722),\n",
       " ('about', 0.9999555349349976),\n",
       " ('cant', 0.9999550580978394)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_long = Word2Vec(sentences=sentences_w2v_long, seed=seed, min_count=1, size=vector_size)\n",
    "word2vec_long.wv.most_similar('love')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "human-feature",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save word vectors\n",
    "word_vectors = word2vec.wv\n",
    "word_vectors.save('../models/w2v/word2vec.wordvectors')\n",
    "word_vectors_stem = word2vec_stem.wv\n",
    "word_vectors_stem.save('../models/w2v/word2vec_stem.wordvectors')\n",
    "word2vec_long = word2vec_long.wv\n",
    "word2vec_long.save('../models/w2v/word2vec_long.wordvectors')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-parent",
   "metadata": {},
   "source": [
    "Clean up the dataset to be saved as training and testing datasets.\n",
    "\n",
    "First, make sure the negative and positive comments are even in numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "western-wales",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    6706\n",
       " 0    4349\n",
       " 1    3152\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_trans_stem.rating.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "reduced-consumption",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    3315\n",
       " 0    1889\n",
       " 1    1236\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_cmts.rating.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conservative-agency",
   "metadata": {},
   "source": [
    "Drop neutral sentiment comments, split positive and negative into separate datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daily-czech",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_trans_stem = no_trans_stem.loc[no_trans_stem.rating != -1]\n",
    "stem = stem.loc[stem.rating != -1]\n",
    "no_trans_stem_pos = no_trans_stem.loc[no_trans_stem.rating == 1]\n",
    "no_trans_stem_neg = no_trans_stem.loc[no_trans_stem.rating == 0]\n",
    "stem_pos = stem.loc[stem.rating == 1]\n",
    "stem_neg = stem.loc[stem.rating == 0]\n",
    "long_cmts = long_cmts.loc[long_cmts.rating != -1]\n",
    "long_cmts_neg = long_cmts.loc[long_cmts.rating == 0]\n",
    "long_cmts_pos = long_cmts.loc[long_cmts.rating == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "prescribed-billy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No stem or trans pos length: 3152, no stem or trans neg length: 4349\n",
      "Stem pos length: 3152, stem neg length: 4349\n"
     ]
    }
   ],
   "source": [
    "# Make sure everything is split correctly\n",
    "print('No stem or trans pos length: %d, no stem or trans neg length: %d' % (len(no_trans_stem_pos), len(no_trans_stem_neg)))\n",
    "print('Stem pos length: %d, stem neg length: %d' % (len(stem_pos), len(stem_neg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "lesser-paste",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write these datasets to file\n",
    "no_trans_stem_pos.to_csv('../data/prepared/no_trans_stem_pos.csv', index=False)\n",
    "no_trans_stem_neg.to_csv('../data/prepared/no_trans_stem_neg.csv', index=False)\n",
    "stem_pos.to_csv('../data/prepared/stem_pos.csv', index=False)\n",
    "stem_neg.to_csv('../data/prepared/stem_neg.csv', index=False)\n",
    "long_cmts_pos.to_csv('../data/prepared/long_pos.csv', index=False)\n",
    "long_cmts_neg.to_csv('../data/prepared/long_neg.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
